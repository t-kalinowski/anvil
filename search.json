[{"path":"/AGENTS.html","id":"package-overview","dir":"","previous_headings":"","what":"Package Overview","title":"NA","text":"anvil code transformation framework similar jax R. currently support jit compilation automatic differentiation.","code":""},{"path":[]},{"path":"/AGENTS.html","id":"build-and-install","dir":"","previous_headings":"Development Commands","what":"Build and Install","title":"NA","text":"","code":"# Load the package for development devtools::load_all()  # Install the package devtools::install()  # Build the package (creates tar.gz file) devtools::build()"},{"path":"/AGENTS.html","id":"testing","dir":"","previous_headings":"Development Commands","what":"Testing","title":"NA","text":"","code":"# Run all tests devtools::test()  # Run a specific test file testthat::test_file(\"tests/testthat/test-constant.R\")"},{"path":"/AGENTS.html","id":"documentation","dir":"","previous_headings":"Development Commands","what":"Documentation","title":"NA","text":"","code":"# Generate documentation from roxygen comments devtools::document()"},{"path":"/AGENTS.html","id":"check","dir":"","previous_headings":"Development Commands","what":"Check","title":"NA","text":"","code":"# Run checks for CRAN compliance devtools::check()"},{"path":"/AGENTS.html","id":"development-practices","dir":"","previous_headings":"","what":"Development Practices","title":"NA","text":"Use S7 (object-oriented system) defining types classes. Follow established pattern adding new operations types Add tests tests/testthat/ Document functions roxygen2 comments","code":""},{"path":"/AGENTS.html","id":"project-information","dir":"","previous_headings":"","what":"Project Information","title":"NA","text":"stablehlo (jit interpretation rules) uses 0-based indexing, anvil uses 1-based indexing. implementing jit interpretation rule, convert indices subtracting 1. rules-pullback.R file contains differentiation rules primitive operations. , grad gradient terminal output respect function’s output function return gradient terminal output respect inputs. tests file insts/extra-tests/test-primitives-pullback-torch.R","code":""},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 anvil authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/anvil.html","id":"the-anviltensor","dir":"Articles","previous_headings":"","what":"The AnvilTensor","title":"Get Started","text":"start introducing main data structure, AnvilTensor. essentially like R array, differences: supports data types, different precisions, well unsigned integers. tensor can live different devices, CPU GPU. 0-dimensional tensors can used represent scalars. can create AnvilTensor R objects using nv_tensor. , create 0-dimensional tensor (.e., scalar) holds 16-bit integer CPU. Note creation scalars, can also use nv_scalar shorthand skip specifying shape omit specifying device, CPU default. can also create higher-dimensional tensors, example 2x3 tensor single-precision floating-point numbers. Without specifying data type, default \"f32\" R doubles \"i32\" integers. as_array() function allows convert AnvilTensors back R objects. Note 0-dimensional tensors, result R vector length 1, R arrays 0 dimensions. first, working AnvilTensors may feel bit cumbersome, directly apply functions like regular R arrays.","code":"library(anvil) set.seed(42) nv_tensor(1L, dtype = \"i16\", device = \"cpu\", shape = integer()) ## AnvilTensor  ##  1 ## [ CPUi16{} ] nv_scalar(1L, dtype = \"i16\") ## AnvilTensor  ##  1 ## [ CPUi16{} ] x <- array(1:6, dim = c(2, 3)) y <- nv_tensor(x) y ## AnvilTensor  ##  1 3 5 ##  2 4 6 ## [ CPUi32{2x3} ] as_array(y) ##      [,1] [,2] [,3] ## [1,]    1    3    5 ## [2,]    2    4    6 x + x ##      [,1] [,2] [,3] ## [1,]    2    6   10 ## [2,]    4    8   12 y + y ## Error in `.current_descriptor()`: ## ! No graph is currently being built. Did you forget to use `jit()`?"},{"path":"/articles/anvil.html","id":"jit-compilation","dir":"Articles","previous_headings":"","what":"JIT Compilation","title":"Get Started","text":"order work AnvilTensors, need convert function want apply jit-compiled version via jit(). result operation AnvilTensor. can also jit-compile complex functions. , define function takes data matrix X, weight vector beta, scalar bias alpha, computes linear model output \\(y = X \\times \\beta + \\alpha\\). One restriction {anvil} function re-compiled every unique combination input types, consisting specific shape data type. demonstrate , create slightly modified version previous linear predictor function: Next, create little helper function creates example inputs different numbers observations: , call function twice data shapes. can notice see \"compiling ...\" message first time, function first compiled XLA executable, cached, executed. second time, executable retrieved cache (inputs shapes data types) executed without recompilation. executable contains operations applied AnvilTensors, contain cat() call, don’t see second time. call function data different shapes (data types), function re-compiled message re-appears. compilation step can take time, {anvil} therefore gives best results function called many times (different) input types, computation sufficiently large amortize compilation overhead.","code":"plus_jit <- jit(`+`) plus_jit(y, y) ## AnvilTensor  ##   2  6 10 ##   4  8 12 ## [ CPUi32{2x3} ] linear_model_r <- function(X, beta, alpha) {   X %*% beta + alpha }  linear_model <- jit(linear_model_r)  X <- nv_tensor(rnorm(6), dtype = \"f32\", shape = c(2, 3)) beta <- nv_tensor(rnorm(3), dtype = \"f32\", shape = c(3, 1)) alpha <- nv_scalar(rnorm(1), dtype = \"f32\")  linear_model(X, beta, alpha) ## AnvilTensor  ##   2.7911 ##  -1.1904 ## [ CPUf32{2x1} ] linear_model2 <- jit(function(X, beta, alpha) {   cat(\"compiling ...\\n\")   X %*% beta + alpha }) simul_data <- function(n, p) {   list(     X = nv_tensor(rnorm(n * p), dtype = \"f32\", shape = c(n, p)),     beta = nv_tensor(rnorm(p), dtype = \"f32\", shape = c(p, 1)),     alpha = nv_scalar(rnorm(1), dtype = \"f32\")   ) } do.call(linear_model2, simul_data(2, 3)) ## compiling ... ## AnvilTensor  ##   4.9640 ##  -0.1413 ## [ CPUf32{2x1} ] do.call(linear_model2, simul_data(2, 3)) ## AnvilTensor  ##   0.6140 ##  -2.5214 ## [ CPUf32{2x1} ] y_hat <- do.call(linear_model2, simul_data(4, 3)) ## compiling ..."},{"path":"/articles/anvil.html","id":"static-arguments","dir":"Articles","previous_headings":"JIT Compilation","what":"Static Arguments","title":"Get Started","text":"Besides AnvilTensors, jit-compiled functions can also take regular R values arguments. example, might want linear model without bias term. , add logical(1) argument with_bias function. need mark argument static, {anvil} knows treat regular R value. can now call function without bias term: Static arguments work differently AnvilTensors function re-compiled new observed value static argument, unique input type combination.","code":"linear_model3 <- jit(function(X, beta, alpha = NULL, with_bias) {   if (with_bias) {     cat(\"Compiling with bias ...\\n\")     X %*% beta + alpha   } else {     cat(\"Compiling without bias ...\\n\")     X %*% beta   } }, static = \"with_bias\") linear_model3(X, beta, with_bias = FALSE) ## Compiling without bias ... ## AnvilTensor  ##   2.8538 ##  -1.1277 ## [ CPUf32{2x1} ] linear_model3(X, beta, alpha, with_bias = TRUE) ## Compiling with bias ... ## AnvilTensor  ##   2.7911 ##  -1.1904 ## [ CPUf32{2x1} ]"},{"path":"/articles/anvil.html","id":"nested-inputs-and-outputs","dir":"Articles","previous_headings":"JIT Compilation","what":"Nested Inputs and Outputs","title":"Get Started","text":"Inputs outputs can also nested data structures contain AnvilTensors, although currently support (named) lists. far, implemented prediction step linear model. One core applications {anvil} implement learning algorithms, often need gradients, well control flow. start gradients.","code":"linear_model4 <- jit(function(inputs) {   list(y_hat = inputs[[1]] %*% inputs[[2]] + inputs[[3]]) }) linear_model4(list(X, beta, alpha)) ## $y_hat ## AnvilTensor  ##   2.7911 ##  -1.1904 ## [ CPUf32{2x1} ]"},{"path":"/articles/anvil.html","id":"automatic-differentiation","dir":"Articles","previous_headings":"","what":"Automatic Differentiation","title":"Get Started","text":"{anvil}, can easily obtain gradient function scalar-valued function via gradient(). Currently, don’t support jacobians hessians, hopefilly added future. , implement loss function linear model. now need target variables y, simulate data linear model:  Next, randomly initialize model parameters: can now define function prediction calculates loss. Note calling original R function prediction jit-compiled version. Using gradient() transformation, can automatically obtain gradient function model_loss respect arguments, specify. Finally, define update step weights using gradient descent. already allows us fit linear model  might seem like reasonable solution, continuously switches R interpreter XLA runtime. Moreover, allocate new tensors iteration weights. latter might big problem small models, can cause significant overhead working bigger tensors. Next, discuss control flow {anvil} address immutability.","code":"mse <- function(y_hat, y) {   mean((y_hat - y)^2.0) } beta <- rnorm(1) X <- matrix(rnorm(100), ncol = 1) alpha <- rnorm(1) y <- X %*% beta + alpha + rnorm(100, sd = 0.5) plot(X, y) X <- nv_tensor(X) y <- nv_tensor(y) beta_hat <- nv_tensor(rnorm(1), shape = c(1, 1), dtype = \"f32\") alpha_hat <- nv_scalar(rnorm(1), dtype = \"f32\") model_loss <- function(X, beta, alpha, y) {   y_hat <- linear_model_r(X, beta, alpha)   mse(y_hat, y) } model_loss_grad <- gradient(   model_loss,   wrt = c(\"beta\", \"alpha\") ) update_weights_r <- function(X, beta, alpha, y) {   lr <- 0.1   grads <- model_loss_grad(X, beta, alpha, y)   beta_new <- beta - lr * grads$beta   alpha_new <- alpha - lr * grads$alpha   list(beta = beta_new, alpha = alpha_new) } update_weights <- jit(update_weights_r) weights <- list(beta = beta_hat, alpha = alpha_hat) for (i in 1:100) {   weights <- update_weights(X, weights$beta, weights$alpha, y) }"},{"path":"/articles/anvil.html","id":"control-flow","dir":"Articles","previous_headings":"","what":"Control Flow","title":"Get Started","text":"principle, three ways implement control-flow {anvil}: Embed jit-compiled functions inside R control-flow constructs, seen earlier. Embed R control flow inside jit-compiled function (also seen earlier linear model allowed optionally include bias term). Use special control-flow primitives provided anvil, nv_while() nv_if(). solution best depends specific scenario, cover three cases, risk bit repetitive. illustrate linear model training example earlier. first implementation already seen earlier: jit-compile update step repeatedly call R loop: simple update steps, solution can inefficient every call jit-compiled function overhead. significant overhead depends expensive call loop – expensive functions overhead becomes negligible. second approach use R loop within jit-compiled function. , loop unrolled compilation step (conditionals, one branch included executable discussed earlier). rather slow example hand, also re-compute gradient function iteration. Moreover, parameter n_steps static, means every unique value n_steps, function re-compiled different executable. Finally, third approach use nv_while function. like standard loop, anvil purely functional. function takes : initial state, (nested) list AnvilTensors. cond function, takes input current state returns logical flag indicating whether continue loop. body function, takes input current state returns new state. approach works analogously -statements, {anvil} primitive nv_if available.","code":"n_steps <- 100L beta_hat <- nv_tensor(rnorm(1), shape = c(1, 1), dtype = \"f32\") alpha_hat <- nv_scalar(rnorm(1), dtype = \"f32\")  weights <- list(beta = beta_hat, alpha = alpha_hat) for (i in seq_len(n_steps)) {   weights <- update_weights(X, weights$beta, weights$alpha, y) } weights ## $beta ## AnvilTensor  ##  -0.9184 ## [ CPUf32{1x1} ]  ##  ## $alpha ## AnvilTensor  ##  -0.4376 ## [ CPUf32{} ] train_unrolled <- jit(function(X, beta, alpha, y, n_steps) {   lr <- nv_scalar(0.1)   for (i in seq_len(n_steps)) {     grads <- model_loss_grad(X, beta, alpha, y)     beta <- beta - lr * grads$beta     alpha <- alpha - lr * grads$alpha   }   list(beta = beta, alpha = alpha) }, static = \"n_steps\")  beta_hat <- nv_tensor(rnorm(1), shape = c(1, 1), dtype = \"f32\") alpha_hat <- nv_scalar(rnorm(1), dtype = \"f32\") train_unrolled(X, beta_hat, alpha_hat, y, n_steps = 10L) ## $beta ## AnvilTensor  ##  -0.9600 ## [ CPUf32{1x1} ]  ##  ## $alpha ## AnvilTensor  ##  -0.3703 ## [ CPUf32{} ] train_while <- jit(function(X, beta, alpha, y, n_steps) {   lr <- 0.1   nv_while(     list(beta = beta, alpha = alpha, i = nv_scalar(0L)),     \\(beta, alpha, i) i < n_steps,     \\(beta, alpha, i) {       grads <- model_loss_grad(X, beta, alpha, y)       list(         beta = beta - lr * grads$beta,         alpha = alpha - lr * grads$alpha,         i = i + 1L       )     }   ) })  beta_hat <- nv_tensor(rnorm(1), shape = c(1, 1), dtype = \"f32\") alpha_hat <- nv_scalar(rnorm(1), dtype = \"f32\") train_while(X, beta_hat, alpha_hat, y, nv_scalar(100L)) ## $beta ## AnvilTensor  ##  -0.9184 ## [ CPUf32{1x1} ]  ##  ## $alpha ## AnvilTensor  ##  -0.4376 ## [ CPUf32{} ]  ##  ## $i ## AnvilTensor  ##  100 ## [ CPUi32{} ]"},{"path":"/articles/anvil.html","id":"immutability","dir":"Articles","previous_headings":"","what":"Immutability","title":"Get Started","text":"AnvilTensor objects immutable, .e., created, value changed. words, conceptually -place updates like x[1] <- x[1] + 1. means {anvil} follows value semantics, .e., functions pure. Naturally, raises question impacts performance. need distinguish two scenarios: Updating AnvilTensor “lives within” jit-compiled function. Updating AnvilTensor “living R” jit-compiled function. first scenario, nothing worry . XLA compiler able optimize , ensuring unnecessary copies actually made. similar copy--write semantics R. evaluate x <- 1:10; y <- x, conceptually creating copy x assigning y, internally, optimized away copy created modifying y x. {anvil} uses compilation shapes known compile time, can make many optimizations, minimizing unnecessary copies much possible. However, also case one calls {anvil} function R code, done initial linear model example. assignment outputs {anvil} function R variables happen within executable, XLA compiler optimize . know inputs {anvil} function needed anymore function call (case “update-calls” like ), can mark “donatable” jit-compiling. tell XLA compiler longer need inputs alpha beta afterwards, underlying memory can reused. now print input weights, get error tensors deleted. new weights still .","code":"weights <- update_weights(X, weights$beta, weights$alpha, y) update_weights_donatable <- jit(update_weights_r, donate = c(\"beta\", \"alpha\")) weights_out <- update_weights_donatable(X, weights$beta, weights$alpha, y) weights ## $beta ## AnvilTensor ## Error: ToLiteral() called on deleted or donated buffer: INVALID_ARGUMENT: Buffer has been deleted or donated. weights_out ## $beta ## AnvilTensor  ##  -0.9184 ## [ CPUf32{1x1} ]  ##  ## $alpha ## AnvilTensor  ##  -0.4376 ## [ CPUf32{} ]"},{"path":"/articles/internals.html","id":"transformations-under-the-hood","dir":"Articles","previous_headings":"","what":"Transformations under the Hood","title":"Internals","text":"Just like real anvil, {anvil} package tool allows reshape objects. former used shape metal, {anvil} package can used transform code code. general, three types transformations: R -> Graph: Generic R functions way complicated handle, first step {anvil} always convert computational anvil::Graph object via tracing. Graph similar JAXExpr objects JAX. Graph -> Graph: possible transform Graphs Graphs. Currently, one transformation, gradient() transformation. Graph -> Executable: order perform actual computation, Graph needs converted executable. Currently, support XLA backend (via stablehlo pjrt), principle possible support backends well.","code":""},{"path":"/articles/internals.html","id":"tracing-r-functions-into-graphs","dir":"Articles","previous_headings":"Transformations under the Hood","what":"Tracing R functions into Graphs","title":"Internals","text":"functionality {anvil} package centered around Graph class, Graphs created tracing R functions. general, want convert code another form, two approaches: Static analysis, require operating abstract syntax tree (AST) code. Dynamic analysis (aka “tracing”), executes code records (selected) operations. former approach followed {quickr} package, allows transpile R code Fortran. {anvil}, following dynamic approach, now illustrate. goal trace following function, either adds subtracts two tensors. first two arguments expected AnvilTensors, third either \"add\" \"sub\". , use anvil::trace_fn(), takes R function list example arguments specify types inputs. output trace_fn() now Graph object represents computation. fields Graph : inputs, GraphValues represent inputs function. outputs, GraphValues represent outputs function. calls, PrimitiveCalls take GraphValues (parameters) produce output GraphValues. in_tree, out_tree, cover later (??) happens internally trace_fn() new GraphDescriptor created inputs x y converted anvil::GraphBox objects. , function f simply evaluated GraphBox objects inputs. evaluation, need distinguish two cases: “standard” R function called: , nothing special happens function simply evaluated. anvil function called: , operation underlies function recorded GraphDescriptor. evaluation statement example first category. set op = \"add\", first branch executed. , calling nv_add, attaches PrimitiveCall represents addition two tensors @calls GraphDescriptor. PrimitiveCall object consists following fields: primitive: primitive function called. inputs: inputs primitive function. params: parameters (non-tensors) primitive function. outputs: outputs primitive function. evaluation f complete, @outputs field GraphDescriptor set Graph subsequently created GraphDescriptor. difference Graph GraphDescriptor latter utility fields useful graph creation, purposes tutorial, can think .","code":"library(anvil) f <- function(x, y, op) {   if (op == \"add\") {     nv_add(x, y)   } else if (op == \"sub\") {     nv_sub(x, y)   } else {     stop(\"Unsupported operation\")   } } x <- nv_scalar(1, \"f32\") y <- nv_scalar(5, \"f32\") graph <- trace_fn(f, list(x = x, y = y, op = \"add\")) graph ## <Graph> ##   Inputs: ##     %x1: f32[] ##     %x2: f32[] ##   Body: ##     %1: f32[] = add(%x1, %x2) ##   Outputs: ##     %1: f32[]"},{"path":"/articles/internals.html","id":"transforming-graphs-into-other-graphs","dir":"Articles","previous_headings":"Transformations under the Hood","what":"Transforming Graphs into other Graphs","title":"Internals","text":"staged R function simpler format, can sorts things , e.g., compute gradient. {anvil} package way dictate Graph Graph transformation can implemented. interesting transformations, however, need store information {anvil} primitive function. case gradient, need store derivative rules. , anvil::Primitive objects @rules field can populated. derivative rules stored functions \"backward\" name. anvil:::transform_gradient function uses rules compute gradient function. specific transformation, walking graph backwards apply derivative rules, add “backward pass” graph. Besides forward graph, transformation takes wrt argument, specifies respect arguments compute gradient.","code":"anvil:::p_add@rules[[\"backward\"]] ## function (inputs, outputs, grads, .required)  ## { ##     grad <- grads[[1L]] ##     list(if (.required[[1L]]) grad, if (.required[[2L]]) grad) ## } ## <bytecode: 0x562df1d7bc58> ## <environment: namespace:anvil> bwd_graph <- anvil:::transform_gradient(graph, wrt = c(\"x\", \"y\")) bwd_graph ## <Graph> ##   Inputs: ##     %x1: f32[] ##     %x2: f32[] ##   Constants: ##     %c1: f32[] ##   Body: ##     %1: f32[] = add(%x1, %x2) ##   Outputs: ##     %c1: f32[] ##     %c1: f32[]"},{"path":"/articles/internals.html","id":"lowering-a-graph","dir":"Articles","previous_headings":"Transformations under the Hood","what":"Lowering a Graph","title":"Internals","text":"order execute Graph, need convert – wait – executable. Currently, one way , namely convert Graph stablehlo::Func object compile via pjrt::pjrt_compile(). Like gradient transformation, rules transformation stored @rules fields primitives. applied anvil::stablehlo function. Now, can compile function via pjrt_compile(). run function, simply pass tensors executable.","code":"anvil:::p_add@rules[[\"stablehlo\"]] ## function (lhs, rhs)  ## { ##     list(stablehlo::hlo_add(lhs, rhs)) ## } ## <bytecode: 0x562df1d86e10> ## <environment: namespace:anvil> func <- stablehlo(graph)[[1L]] func ## func.func @main (%0: tensor<f32>, %1: tensor<f32>) -> tensor<f32> { ## %2 = \"stablehlo.add\" (%0, %1): (tensor<f32>, tensor<f32>) -> (tensor<f32>) ## \"func.return\"(%2): (tensor<f32>) -> () ## } hlo_str <- stablehlo::repr(func) program <- pjrt::pjrt_program(src = hlo_str, format = \"mlir\") exec <- pjrt::pjrt_compile(program) out <- pjrt::pjrt_execute(exec, x, y) out ## PJRTBuffer  ##  6.0000 ## [ CPUf32{} ]"},{"path":[]},{"path":"/articles/internals.html","id":"jit","dir":"Articles","previous_headings":"The user interface","what":"jit","title":"Internals","text":"previous section, shown transformations implemented hood. actual user interface little user-friendly follows JAX interface. start explaining jit(), mark op argument static, .e., AnvilTensor. might intuitive think jit() first calls trace_fn(), runs stablehlo(), followed pjrt_compile(). , however, happening, , need example inputs function. function f_jit instead seen recipe Just Time (JIT). However, apply steps every time f_jit function called, inefficient, tracing compiling takes time. Therefore, function f_jit also contains cache (implemented xlamisc::LRUCache), check whether already executable given inputs. , types AnvilTensors need match exactly (data type shape) static arguments need identical. example, run function AnvilTensors type, different values, function won’t recompiled, can see checking size cache: execute function tensors different dtype shape, function recompiled: Also, provide different values static arguments, function recompiled:","code":"f_jit <-  jit(f, static = \"op\") f_jit(x, y, \"add\") ## AnvilTensor  ##  6.0000 ## [ CPUf32{} ] cache_size <- function(f) environment(f)$cache$size cache_size(f_jit) ## [1] 1 f_jit(nv_scalar(-99, \"f32\"), nv_scalar(2, \"f32\"), \"add\") ## AnvilTensor  ##  -97.0000 ## [ CPUf32{} ] cache_size(f_jit) ## [1] 1 f_jit(nv_scalar(1, \"i32\"), nv_scalar(2, \"i32\"), \"add\") ## AnvilTensor  ##  3 ## [ CPUi32{} ] cache_size(f_jit) ## [1] 2 f_jit(nv_scalar(1, \"f32\"), nv_scalar(2, \"f32\"), \"sub\") ## AnvilTensor  ##  -1.0000 ## [ CPUf32{} ] cache_size(f_jit) ## [1] 3"},{"path":"/articles/internals.html","id":"gradient","dir":"Articles","previous_headings":"The user interface","what":"gradient","title":"Internals","text":"Just like jit(), gradient() also returns function lazily create graph transform , inputs provided. However, output g() called AnvilTensors, just graph building function. order execute , need wrap jit() call: Moreover, can also use g() another function: , happening ? inputs x y provided h_jit, new GraphDescriptor created inputs x y converted GraphBox objects. , record addition x y GraphDescriptor. calling g, second GraphDescriptor created, first build Graph representing g function. done , transform graph gradient graph. done, sub-graph inlined parent graph (far holds addition x y). can look graph : , graph lowered stablehlo subsequently compiled.","code":"g <- gradient(f, wrt = c(\"x\", \"y\")) g_jit <- jit(g, static = \"op\") g_jit(x, y, \"add\") ## $x ## AnvilTensor  ##  1.0000 ## [ CPUf32{} ]  ##  ## $y ## AnvilTensor  ##  1.0000 ## [ CPUf32{} ] h <- function(x, y) {   z <- nv_add(x, y)   g(z, z, \"add\") } h_jit <- jit(h) h_jit(x, y) ## $x ## AnvilTensor  ##  2.0000 ## [ CPUf32{} ]  ##  ## $y ## AnvilTensor  ##  2.0000 ## [ CPUf32{} ] h_graph <- trace_fn(h, list(x = x, y = y))"},{"path":[]},{"path":[]},{"path":[]},{"path":"/articles/type-promotion.html","id":"type-promotion-rules","dir":"Articles","previous_headings":"","what":"Type Promotion Rules","title":"Type Promotion","text":"combining tensors different types (e.g., adding f32 i32), {anvil} needs determine common type. example, adding f32 f64, former promoted latter’s type, ’s expressive. type-promotion rules inspired JAX, designed execution accelerators like GPUs, one often wants speed instead precision. rules defined common_dtype() function. returns list() two values: common dtype flag indicating whether result ambiguous, cover later. table promotion rules . Type promotion rules (row × column)","code":"library(anvil) jit(nv_add)(   nv_scalar(1.0, dtype = \"f32\"),   nv_scalar(1.0, dtype = \"f64\") ) ## AnvilTensor  ##  2.0000 ## [ CPUf64{} ] common_dtype(\"f64\", \"f32\")$dtype ## <stablehlo::FloatType> ##  @ value: int 64 common_dtype(\"i64\", \"f32\")$dtype ## <stablehlo::FloatType> ##  @ value: int 32"},{"path":"/articles/type-promotion.html","id":"literals-as-ambiguous-types","dir":"Articles","previous_headings":"","what":"Literals as Ambiguous Types","title":"Type Promotion","text":"Usually, types {anvil} program can deterministically inferred input types. case possible use R literals. default types literals follows: double() -> dtype(\"f32\") integer() -> dtype(\"i32\") logical() -> dtype(\"i1\") (bool) However, just guess, behave differently known types promotion. Therefore, common_dtype function two arguments indicating data types ambiguous. , first type known f64 second ambiguous f32. Within anvil, denote latter i32?. result f64, although promote f64 known. types ambiguous, result generally known. promotion rules change one type ambiguous . , usually promote ambiguous type known type, unless: ambiguous type float known type . known type bool ambiguous type . case, promote known type default type ambiguous type. table shows promotion rules, rows ambiguous columns known. Promotion rules: ambiguous (row) × known (column)","code":"jit(\\() list(1L, 1.0, TRUE))() ## [[1]] ## AnvilTensor  ##  1 ## [ CPUi32{} ]  ##  ## [[2]] ## AnvilTensor  ##  1.0000 ## [ CPUf32{} ]  ##  ## [[3]] ## AnvilTensor  ##  1 ## [ CPUpred{} ] common_dtype(\"f32\", \"f64\", FALSE, TRUE) ## $dtype ## <stablehlo::FloatType> ##  @ value: int 32 ##  ## $ambiguous ## [1] FALSE common_dtype(\"f32\", \"f64\", TRUE, TRUE) ## $dtype ## <stablehlo::FloatType> ##  @ value: int 64 ##  ## $ambiguous ## [1] TRUE common_dtype(\"f32\", \"f64\", FALSE, FALSE) ## $dtype ## <stablehlo::FloatType> ##  @ value: int 64 ##  ## $ambiguous ## [1] FALSE"},{"path":"/articles/type-promotion.html","id":"propagating-ambiguity","dir":"Articles","previous_headings":"","what":"Propagating Ambiguity","title":"Type Promotion","text":"Ambiguity propagated operations. Consider following example: type z i32?, x promoted i32, default type 1L literal. z ambiguous, output i32, y promoted i32 multiplication. propagate ambiguity, z actually -promoted i16, z ambiguous, y known.","code":"f <- jit(function(x, y) {   z <- x + 1L   z * y }) f(nv_scalar(TRUE), nv_scalar(2L, dtype = \"i16\")) ## AnvilTensor  ##  4 ## [ CPUi16{} ]"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Sebastian Fischer. Maintainer, author. Daniel Falbel. Author. Nikolai German. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Fischer S, Falbel D, German N (2025). anvil: Framework R code transformations. R package version 0.0.0.9000, https://r-xla.github.io/anvil/.","code":"@Manual{,   title = {anvil: Framework for R code transformations},   author = {Sebastian Fischer and Daniel Falbel and Nikolai German},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://r-xla.github.io/anvil/}, }"},{"path":"/index.html","id":"anvil","dir":"","previous_headings":"","what":"Framework for R code transformations","title":"Framework for R code transformations","text":"Composable code transformation framework R, allowing run numerical programs speed light. currently implements JIT compilation fast execution backward-mode automatic differentiation. Programs can run various hardware backends, including CPU GPU.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Framework for R code transformations","text":"order install source, need C++20 compiler, well libprotobuf protobuf-compiler. can also install r-universe, adding code .Rprofile.","code":"pak::pak(\"r-xla/anvil\") options(repos = c(   rxla = \"https://r-xla.r-universe.dev\",   CRAN = \"https://cloud.r-project.org/\" ))"},{"path":"/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick Start","title":"Framework for R code transformations","text":", create standard R function. directly call function, first need wrap jit() call. resulting function called AnvilTensors – primary data type {anvil} – JIT compiled subsequently executed. automatic differentiation, can also obtain gradient function.","code":"library(anvil) f <- function(a, b, x) {   a * x + b } f_jit <- jit(f)  a <- nv_scalar(1.0) b <- nv_scalar(-2.0) x <- nv_scalar(3.0)  f_jit(a, b, x) #> AnvilTensor  #>  1.0000 #> [ CPUf32{} ] g_jit <- jit(gradient(f, wrt = c(\"a\", \"b\"))) g_jit(a, b, x) #> $a #> AnvilTensor  #>  3.0000 #> [ CPUf32{} ]  #>  #> $b #> AnvilTensor  #>  1.0000 #> [ CPUf32{} ]"},{"path":"/index.html","id":"main-features","dir":"","previous_headings":"","what":"Main Features","title":"Framework for R code transformations","text":"Gradients functions scalar outputs supported. Code JIT compiled single kernel. Runs different hardware backends, including CPU GPU. possible add new primitives, transformations, (effort) new backends. package written almost entirely R.","code":""},{"path":"/index.html","id":"when-to-use-this-package","dir":"","previous_headings":"","what":"When to use this package?","title":"Framework for R code transformations","text":"{anvil} allows run certain types programs extremely fast, applies certain category problems. Specifically, suitable numerical algorithms, optimizing bayesian models, training neural networks generally numerical optimization. Another restriction {anvil} needs re-compile code new unique input shape. advantage, compiler can make memory optimizations, compilation overhead might problem fast running programs.","code":""},{"path":"/index.html","id":"acknowledgments","dir":"","previous_headings":"","what":"Acknowledgments","title":"Framework for R code transformations","text":"work supported MaRDI. JAX, especially autodidax tutorial. microjax project. JIT compilation, leverage OpenXLA project.","code":""},{"path":"/reference/AbstractTensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Abstract Tensor Class — AbstractTensor","title":"Abstract Tensor Class — AbstractTensor","text":"Abstract representation tensor (possibly ambiguous) dtype shape, concrete data. Used tracing represent tensor metadata without actual values.","code":""},{"path":"/reference/AbstractTensor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Abstract Tensor Class — AbstractTensor","text":"","code":"AbstractTensor(dtype, shape, ambiguous = FALSE)"},{"path":"/reference/AbstractTensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Abstract Tensor Class — AbstractTensor","text":"dtype (stablehlo::TensorDataType) data type tensor. shape (stablehlo::Shape | integer()) shape tensor. Can provided integer vector. ambiguous (logical(1)) Whether type ambiguous. Ambiguous usually arise R literals (e.g., 1L, 1.0) follow special promotion rules. f32, i32 ambiguous tracing.","code":""},{"path":"/reference/AbstractTensor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Abstract Tensor Class — AbstractTensor","text":"Two tensors considered equal (==) dtype shape, ignoring ambiguity.","code":""},{"path":[]},{"path":"/reference/ConcreteTensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Concrete Tensor Class — ConcreteTensor","title":"Concrete Tensor Class — ConcreteTensor","text":"AbstractTensor also holds reference actual tensor data. Used represent constants captured tracing. comes concrete tensor, type never ambiguous.","code":""},{"path":"/reference/ConcreteTensor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Concrete Tensor Class — ConcreteTensor","text":"","code":"ConcreteTensor(data)"},{"path":"/reference/ConcreteTensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Concrete Tensor Class — ConcreteTensor","text":"data (AnvilTensor) actual tensor data.","code":""},{"path":[]},{"path":"/reference/Graph.html","id":null,"dir":"Reference","previous_headings":"","what":"Graph of Primitive Calls — Graph","title":"Graph of Primitive Calls — Graph","text":"Computational graph consisting exclusively primitive calls.","code":""},{"path":"/reference/Graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Graph of Primitive Calls — Graph","text":"","code":"Graph(   calls = list(),   in_tree = NULL,   out_tree = NULL,   inputs = list(),   outputs = list(),   constants = list() )"},{"path":"/reference/Graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Graph of Primitive Calls — Graph","text":"calls (list(PrimitiveCall)) primitive calls make graph. can also another call graph primitive p_call. in_tree (NULL | Node) tree inputs. out_tree (NULL | Node) tree outputs. inputs (list(GraphValue)) inputs graph. outputs (list(GraphValue)) outputs graph. constants (list(GraphValue)) constants graph.","code":""},{"path":"/reference/GraphDescriptor.html","id":null,"dir":"Reference","previous_headings":"","what":"Graph Descriptor — GraphDescriptor","title":"Graph Descriptor — GraphDescriptor","text":"Descriptor Graph.","code":""},{"path":"/reference/GraphDescriptor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Graph Descriptor — GraphDescriptor","text":"","code":"GraphDescriptor(   calls = list(),   tensor_to_gval = hashtab(),   gval_to_box = hashtab(),   constants = list(),   in_tree = NULL,   out_tree = NULL,   inputs = list(),   outputs = list() )"},{"path":"/reference/GraphDescriptor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Graph Descriptor — GraphDescriptor","text":"calls (list(PrimitiveCall)) primitive calls make graph. tensor_to_gval (hashtab) Mapping: AnvilTensor -> GraphValue gval_to_box (hashtab) Mapping: GraphValue -> GraphBox constants (list(GraphValue)) constants graph. in_tree (NULL | Node) tree inputs. out_tree (NULL | Node) tree outputs. inputs (list(GraphValue)) inputs graph. outputs (list(GraphValue)) outputs graph.","code":""},{"path":"/reference/GraphDescriptor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Graph Descriptor — GraphDescriptor","text":"trickiest thing setup ensure values receive identifier (GraphValue) across nested graphs. two cases: ","code":""},{"path":"/reference/GraphLiteral.html","id":null,"dir":"Reference","previous_headings":"","what":"Graph Literal — GraphLiteral","title":"Graph Literal — GraphLiteral","text":"Literal Graph.","code":""},{"path":"/reference/GraphLiteral.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Graph Literal — GraphLiteral","text":"","code":"GraphLiteral(aval = LiteralTensor())"},{"path":"/reference/GraphLiteral.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Graph Literal — GraphLiteral","text":"aval () value literal.","code":""},{"path":"/reference/GraphValue.html","id":null,"dir":"Reference","previous_headings":"","what":"Graph Value — GraphValue","title":"Graph Value — GraphValue","text":"Value Graph.","code":""},{"path":"/reference/GraphValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Graph Value — GraphValue","text":"","code":"GraphValue(aval = AbstractTensor())"},{"path":"/reference/GraphValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Graph Value — GraphValue","text":"aval (AbstractTensor) abstract value variable.","code":""},{"path":"/reference/LiteralTensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Literal Tensor Class — LiteralTensor","title":"Literal Tensor Class — LiteralTensor","text":"AbstractTensor representing tensor data R scalar literal (e.g., 1L, 2.5). Usually, type ambiguous, unless created via nv_fill.","code":""},{"path":"/reference/LiteralTensor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Literal Tensor Class — LiteralTensor","text":"","code":"LiteralTensor(data, shape, dtype = default_dtype(data), ambiguous)"},{"path":"/reference/LiteralTensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Literal Tensor Class — LiteralTensor","text":"data (numeric(1) | integer(1) | logical(1)) scalar value. shape (stablehlo::Shape | integer()) shape tensor. dtype (stablehlo::TensorDataType) data type. Defaults f32 numeric, i32 integer, i1 logical. ambiguous (logical(1)) Whether type ambiguous. Ambiguous usually arise R literals (e.g., 1L, 1.0) follow special promotion rules. f32 i32 can ambiguous tracing.","code":""},{"path":[]},{"path":"/reference/Primitive.html","id":null,"dir":"Reference","previous_headings":"","what":"Primitive — Primitive","title":"Primitive — Primitive","text":"Primitive interpretation rule.","code":""},{"path":"/reference/Primitive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Primitive — Primitive","text":"","code":"Primitive(name)"},{"path":"/reference/Primitive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Primitive — Primitive","text":"name (character()) name primitive.","code":""},{"path":"/reference/Primitive.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Primitive — Primitive","text":"(Primitive)","code":""},{"path":"/reference/PrimitiveCall.html","id":null,"dir":"Reference","previous_headings":"","what":"Primitive Call — PrimitiveCall","title":"Primitive Call — PrimitiveCall","text":"Call primitive Graph Note primitive call also call another graph (p_graph).","code":""},{"path":"/reference/PrimitiveCall.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Primitive Call — PrimitiveCall","text":"","code":"PrimitiveCall(   primitive = Primitive(),   inputs = list(),   params = list(),   outputs = list() )"},{"path":"/reference/PrimitiveCall.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Primitive Call — PrimitiveCall","text":"primitive (Primitive) function. inputs (list(GraphValue)) (tensor) inputs primitive. params (list(<>)) (static) parameters function call. outputs (list(GraphValue)) (tensor) outputs primitive.","code":""},{"path":"/reference/common_dtype.html","id":null,"dir":"Reference","previous_headings":"","what":"Type Promotion Rules — common_dtype","title":"Type Promotion Rules — common_dtype","text":"Computes common dtype set abstract types, respecting whether type ambiguous . type ambiguous comes literal (like 1 1.0) promoted ambiguous type. Promoting ambiguous type can happen scenarios like x + 1.2, x bool int.","code":""},{"path":"/reference/common_dtype.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type Promotion Rules — common_dtype","text":"","code":"common_dtype(   lhs_dtype,   rhs_dtype,   lhs_ambiguous = FALSE,   rhs_ambiguous = FALSE )"},{"path":"/reference/common_dtype.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type Promotion Rules — common_dtype","text":"lhs_dtype (stablehlo::TensorDataType) left-hand side type. rhs_dtype (stablehlo::TensorDataType) right-hand side type. lhs_ambiguous (logical(1)) Whether left-hand side type ambiguous. rhs_ambiguous (logical(1)) Whether right-hand side type ambiguous.","code":""},{"path":"/reference/common_dtype.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Type Promotion Rules — common_dtype","text":"(list(dtype = [stablehlo::TensorDataType], ambiguous = logical(1)`)","code":""},{"path":"/reference/dot-current_descriptor.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the current graph — .current_descriptor","title":"Get the current graph — .current_descriptor","text":"Get current graph built (via local_descriptor).","code":""},{"path":"/reference/dot-current_descriptor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the current graph — .current_descriptor","text":"","code":".current_descriptor()"},{"path":"/reference/dot-current_descriptor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the current graph — .current_descriptor","text":"Graph object.","code":""},{"path":"/reference/gradient.html","id":null,"dir":"Reference","previous_headings":"","what":"Gradient — gradient","title":"Gradient — gradient","text":"Transform function gradient.","code":""},{"path":"/reference/gradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gradient — gradient","text":"","code":"gradient(f, wrt = NULL)  value_and_gradient(f, wrt = NULL)"},{"path":"/reference/gradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gradient — gradient","text":"f (function) Function compute gradient . wrt (character) Names arguments compute gradient respect .","code":""},{"path":"/reference/gradient.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Gradient — gradient","text":"value_and_gradient(): Returns value gradient","code":""},{"path":"/reference/graph_to_quickr_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a Graph to a quickr-compiled function — graph_to_quickr_function","title":"Convert a Graph to a quickr-compiled function — graph_to_quickr_function","text":"Lowers supported subset anvil::Graph objects plain R function compiles quickr::quick().","code":""},{"path":"/reference/graph_to_quickr_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a Graph to a quickr-compiled function — graph_to_quickr_function","text":"","code":"graph_to_quickr_function(graph)"},{"path":"/reference/graph_to_quickr_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a Graph to a quickr-compiled function — graph_to_quickr_function","text":"graph (Graph) Graph convert.","code":""},{"path":"/reference/graph_to_quickr_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a Graph to a quickr-compiled function — graph_to_quickr_function","text":"(function)","code":""},{"path":"/reference/graph_to_quickr_function.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert a Graph to a quickr-compiled function — graph_to_quickr_function","text":"returned function expects plain R scalars/vectors/arrays (AnvilTensor) returns plain R values/arrays. graph returns multiple outputs (e.g. nested list), compiled function returns structure packing/unpacking values quickr. moment supports graphs flat (non-nested) argument list. Currently supported primitives : constant, add, sub, mul, divide, negate, broadcast_in_dim, dot_general, transpose, reshape, sum. code generator currently supports tensors rank 5. primitives restricted (e.g. transpose currently handles rank-2 tensors).","code":""},{"path":"/reference/jit.html","id":null,"dir":"Reference","previous_headings":"","what":"JIT compile a function — jit","title":"JIT compile a function — jit","text":"Convert function JIT compiled function.","code":""},{"path":"/reference/jit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"JIT compile a function — jit","text":"","code":"jit(   f,   static = character(),   device = NULL,   cache_size = 100L,   donate = character() )"},{"path":"/reference/jit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"JIT compile a function — jit","text":"f (function) Function compile. static (character()) parameters f static. device (NULL | character(1) | PJRTDevice) device use compiled function. default (NULL) uses PJRT_PLATFORM environment variable defaults \"cpu\". cache_size (integer(1)) size cache jit-compiled functions. donate (character()) Names arguments whose buffers donated. Donated buffers can aliased outputs type, allowing -place operations reducing memory usage.","code":""},{"path":"/reference/jit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"JIT compile a function — jit","text":"(function)","code":""},{"path":"/reference/local_descriptor.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a graph — local_descriptor","title":"Create a graph — local_descriptor","text":"Creates new Graph afterwards accessible via .current_descriptor(). graph automatically removed exiting current scope. graph either cleaned automatically (exiting scope) finalized, previously built graph restored, .e., accessible via .current_descriptor().","code":""},{"path":"/reference/local_descriptor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a graph — local_descriptor","text":"","code":"local_descriptor(..., envir = parent.frame())"},{"path":"/reference/local_descriptor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a graph — local_descriptor","text":"... () Additional arguments pass GraphDescriptor constructor. envir (environment) Environment exit handler registered cleaning Graph returned yet.","code":""},{"path":"/reference/local_descriptor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a graph — local_descriptor","text":"Graph object.","code":""},{"path":"/reference/mut.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an S7 class to a mutable S7 object — mut","title":"Convert an S7 class to a mutable S7 object — mut","text":"Convert S7 class mutable S7 object","code":""},{"path":"/reference/mut.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an S7 class to a mutable S7 object — mut","text":"","code":"mut(x)"},{"path":"/reference/mut.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an S7 class to a mutable S7 object — mut","text":"x S7_class S7_class class constructor - function otherwise call create new object class.","code":""},{"path":"/reference/mut.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert an S7 class to a mutable S7 object — mut","text":"","code":"library(S7)  class_ex <- new_class(   \"class_example\",   properties = list(     # validators     value = new_property(       class = class_integer,       validator = function(value) {         if (value == 0L) \"value cannot be exactly 0\"       }     ),     # read-only properties     is_gt_0 = new_property(       class = class_logical,       getter = function(self) {         self@value > 0       }     )   ) )  # make a mutable version of our class ex <- mut(class_ex)(value = 3L) ex@value #> [1] 3  # we can make a copy and update our value property ex_ref <- ex ex_ref@value <- 30L  # all values reference the same data, our original is updated ex@value #> [1] 30"},{"path":"/reference/nv_binary_ops.html","id":null,"dir":"Reference","previous_headings":"","what":"Binary Operations — nv_binary_ops","title":"Binary Operations — nv_binary_ops","text":"Binary operations tensors.","code":""},{"path":"/reference/nv_binary_ops.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Binary Operations — nv_binary_ops","text":"","code":"nv_add(lhs, rhs)  nv_mul(lhs, rhs)  nv_sub(lhs, rhs)  nv_div(lhs, rhs)  nv_pow(lhs, rhs)  nv_eq(lhs, rhs)  nv_ne(lhs, rhs)  nv_gt(lhs, rhs)  nv_ge(lhs, rhs)  nv_lt(lhs, rhs)  nv_le(lhs, rhs)  nv_max(lhs, rhs)  nv_min(lhs, rhs)  nv_remainder(lhs, rhs)  nv_and(lhs, rhs)  nv_or(lhs, rhs)  nv_xor(lhs, rhs)  nv_shift_left(lhs, rhs)  nv_shift_right_logical(lhs, rhs)  nv_shift_right_arithmetic(lhs, rhs)  nv_atan2(lhs, rhs)"},{"path":"/reference/nv_binary_ops.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Binary Operations — nv_binary_ops","text":"lhs (nv_tensor) rhs (nv_tensor)","code":""},{"path":"/reference/nv_binary_ops.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Binary Operations — nv_binary_ops","text":"nv_tensor","code":""},{"path":"/reference/nv_binary_ops.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Binary Operations — nv_binary_ops","text":"","code":"# Comparison operators such `nv_eq`, `nv_le`, `nv_gt`, etc # are nondifferentiable and contribute zero to gradients. relu <- function(x) {   nv_convert(x > nv_scalar(0), \"f32\")*x } # df/dx = 1 if x > 0 else 0 g_relu <- jit(gradient(relu, \"x\"))  g_relu(nv_scalar(1, dtype = \"f32\")) #> $x #> AnvilTensor  #>  1.0000 #> [ CPUf32{} ]  #>  g_relu(nv_scalar(-1, dtype = \"f32\")) #> $x #> AnvilTensor  #>  0.0000 #> [ CPUf32{} ]  #>"},{"path":"/reference/nv_bitcast_convert.html","id":null,"dir":"Reference","previous_headings":"","what":"Bitcast Conversion — nv_bitcast_convert","title":"Bitcast Conversion — nv_bitcast_convert","text":"Reinterpret Bits","code":""},{"path":"/reference/nv_bitcast_convert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bitcast Conversion — nv_bitcast_convert","text":"","code":"nv_bitcast_convert(operand, dtype)"},{"path":"/reference/nv_bitcast_convert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bitcast Conversion — nv_bitcast_convert","text":"operand tensor dtype requested dtype","code":""},{"path":"/reference/nv_broadcast_scalars.html","id":null,"dir":"Reference","previous_headings":"","what":"Broadcast Scalars to Common Shape — nv_broadcast_scalars","title":"Broadcast Scalars to Common Shape — nv_broadcast_scalars","text":"Broadcast scalar tensors match shape non-scalar tensors. non-scalar tensors must shape.","code":""},{"path":"/reference/nv_broadcast_scalars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Broadcast Scalars to Common Shape — nv_broadcast_scalars","text":"","code":"nv_broadcast_scalars(...)"},{"path":"/reference/nv_broadcast_scalars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Broadcast Scalars to Common Shape — nv_broadcast_scalars","text":"... (nv_tensor) Tensors broadcast. Scalars broadcast common non-scalar shape.","code":""},{"path":"/reference/nv_broadcast_scalars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Broadcast Scalars to Common Shape — nv_broadcast_scalars","text":"(list() nv_tensor)","code":""},{"path":"/reference/nv_broadcast_tensors.html","id":null,"dir":"Reference","previous_headings":"","what":"Broadcast Tensors to a Common Shape — nv_broadcast_tensors","title":"Broadcast Tensors to a Common Shape — nv_broadcast_tensors","text":"Broadcast tensors common shape.","code":""},{"path":"/reference/nv_broadcast_tensors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Broadcast Tensors to a Common Shape — nv_broadcast_tensors","text":"","code":"nv_broadcast_tensors(...)"},{"path":"/reference/nv_broadcast_tensors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Broadcast Tensors to a Common Shape — nv_broadcast_tensors","text":"... (nv_tensor) Tensors broadcast.","code":""},{"path":"/reference/nv_broadcast_tensors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Broadcast Tensors to a Common Shape — nv_broadcast_tensors","text":"(list() nv_tensor)","code":""},{"path":"/reference/nv_broadcast_tensors.html","id":"broadcasting-rules","dir":"Reference","previous_headings":"","what":"Broadcasting Rules","title":"Broadcast Tensors to a Common Shape — nv_broadcast_tensors","text":"follow standard NumPy broadcasting rules: tensors different numbers dimensions, prepend 1s shape smaller tensor. dimension, : sizes , nothing. one tensors size 1, expand corresponding size tensor. sizes different neither 1, raise error.","code":""},{"path":"/reference/nv_broadcast_to.html","id":null,"dir":"Reference","previous_headings":"","what":"Broadcast — nv_broadcast_to","title":"Broadcast — nv_broadcast_to","text":"Broadcast tensor given shape using NumPy broadcasting rules.","code":""},{"path":"/reference/nv_broadcast_to.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Broadcast — nv_broadcast_to","text":"","code":"nv_broadcast_to(operand, shape)"},{"path":"/reference/nv_broadcast_to.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Broadcast — nv_broadcast_to","text":"operand (nv_tensor) Operand. shape (integer()) Output shape.","code":""},{"path":"/reference/nv_broadcast_to.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Broadcast — nv_broadcast_to","text":"(nv_tensor)","code":""},{"path":"/reference/nv_concatenate.html","id":null,"dir":"Reference","previous_headings":"","what":"Concatenate — nv_concatenate","title":"Concatenate — nv_concatenate","text":"Concatenate variadic number tensors.","code":""},{"path":"/reference/nv_concatenate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Concatenate — nv_concatenate","text":"","code":"nv_concatenate(..., dimension)"},{"path":"/reference/nv_concatenate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Concatenate — nv_concatenate","text":"... tensors dimension (integer()) dimension concatenate along . dimensions must .","code":""},{"path":"/reference/nv_concatenate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Concatenate — nv_concatenate","text":"nv_tensor","code":""},{"path":"/reference/nv_convert.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Tensor to Different Data Type — nv_convert","title":"Convert Tensor to Different Data Type — nv_convert","text":"Convert tensor different data type.","code":""},{"path":"/reference/nv_convert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Tensor to Different Data Type — nv_convert","text":"","code":"nv_convert(operand, dtype)"},{"path":"/reference/nv_convert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Tensor to Different Data Type — nv_convert","text":"operand (nv_tensor) Operand. dtype (character(1) | stablehlo::TensorDataType) Data type.","code":""},{"path":"/reference/nv_convert.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Tensor to Different Data Type — nv_convert","text":"nv_tensor","code":""},{"path":"/reference/nv_fill.html","id":null,"dir":"Reference","previous_headings":"","what":"Constant — nv_fill","title":"Constant — nv_fill","text":"Create constant.","code":""},{"path":"/reference/nv_fill.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constant — nv_fill","text":"","code":"nv_fill(value, shape, dtype = NULL)"},{"path":"/reference/nv_fill.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constant — nv_fill","text":"value () Value. shape (integer()) Shape. dtype (character(1)) Data type.","code":""},{"path":"/reference/nv_if.html","id":null,"dir":"Reference","previous_headings":"","what":"If — nv_if","title":"If — nv_if","text":"Functional statement.","code":""},{"path":"/reference/nv_if.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"If — nv_if","text":"","code":"nv_if(pred, true, false)"},{"path":"/reference/nv_if.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"If — nv_if","text":"pred (nv_tensor) Flag. true (NSE) Expression evaluate condition true. false (NSE) Expression evaluate condition false.","code":""},{"path":"/reference/nv_if.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"If — nv_if","text":"nv_tensor","code":""},{"path":"/reference/nv_matmul.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix Multiplication — nv_matmul","title":"Matrix Multiplication — nv_matmul","text":"Matrix multiplication two tensors.","code":""},{"path":"/reference/nv_matmul.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix Multiplication — nv_matmul","text":"","code":"nv_matmul(lhs, rhs)"},{"path":"/reference/nv_matmul.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix Multiplication — nv_matmul","text":"lhs (nv_tensor) rhs (nv_tensor)","code":""},{"path":"/reference/nv_matmul.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matrix Multiplication — nv_matmul","text":"nv_tensor","code":""},{"path":"/reference/nv_matmul.html","id":"shapes","dir":"Reference","previous_headings":"","what":"Shapes","title":"Matrix Multiplication — nv_matmul","text":"lhs: (b1, ..., bk, m, n) rhs: (b1, ..., bk, n, p) output: (b1, ..., bk, m, p)","code":""},{"path":"/reference/nv_matmul.html","id":"broadcasting","dir":"Reference","previous_headings":"","what":"Broadcasting","title":"Matrix Multiplication — nv_matmul","text":"dimensions last two broadcasted.","code":""},{"path":"/reference/nv_promote_to_common.html","id":null,"dir":"Reference","previous_headings":"","what":"Promote Tensors to a Common Dtype — nv_promote_to_common","title":"Promote Tensors to a Common Dtype — nv_promote_to_common","text":"Promote tensors common data type, see common_dtype details.","code":""},{"path":"/reference/nv_promote_to_common.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Promote Tensors to a Common Dtype — nv_promote_to_common","text":"","code":"nv_promote_to_common(...)"},{"path":"/reference/nv_promote_to_common.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Promote Tensors to a Common Dtype — nv_promote_to_common","text":"... (nv_tensor) Tensors promote.","code":""},{"path":"/reference/nv_promote_to_common.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Promote Tensors to a Common Dtype — nv_promote_to_common","text":"(list() nv_tensor)","code":""},{"path":"/reference/nv_reduce_ops.html","id":null,"dir":"Reference","previous_headings":"","what":"Reduction Operators — nv_reduce_ops","title":"Reduction Operators — nv_reduce_ops","text":"Reduce tensor along specified dimensions.","code":""},{"path":"/reference/nv_reduce_ops.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reduction Operators — nv_reduce_ops","text":"","code":"nv_reduce_sum(operand, dims, drop = TRUE)  nv_reduce_mean(operand, dims, drop = TRUE)  nv_reduce_prod(operand, dims, drop = TRUE)  nv_reduce_max(operand, dims, drop = TRUE)  nv_reduce_min(operand, dims, drop = TRUE)  nv_reduce_any(operand, dims, drop = TRUE)  nv_reduce_all(operand, dims, drop = TRUE)"},{"path":"/reference/nv_reduce_ops.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reduction Operators — nv_reduce_ops","text":"operand (nv_tensor) Operand. dims (integer()) Dimensions reduce. drop (logical(1)) Whether drop reduced dimensions.","code":""},{"path":"/reference/nv_reduce_ops.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reduction Operators — nv_reduce_ops","text":"nv_tensor","code":""},{"path":"/reference/nv_reshape.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape — nv_reshape","title":"Reshape — nv_reshape","text":"Reshape tensor. Note row-major order used, differs R's column-major order.","code":""},{"path":"/reference/nv_reshape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape — nv_reshape","text":"","code":"nv_reshape(operand, shape)"},{"path":"/reference/nv_reshape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape — nv_reshape","text":"operand (nv_tensor) Operand. shape (integer()) new shape.","code":""},{"path":"/reference/nv_reshape.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape — nv_reshape","text":"nv_tensor","code":""},{"path":"/reference/nv_rng_bit_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"Random Numbers — nv_rng_bit_generator","title":"Random Numbers — nv_rng_bit_generator","text":"generate random bits desired shape dtype","code":""},{"path":"/reference/nv_rng_bit_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random Numbers — nv_rng_bit_generator","text":"","code":"nv_rng_bit_generator(   initial_state,   rng_algorithm = \"THREE_FRY\",   dtype,   shape_out )"},{"path":"/reference/nv_rng_bit_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random Numbers — nv_rng_bit_generator","text":"initial_state state seed rng_algorithm one 'DEFAULT', 'THREE_FRY', 'PHILOX' dtype datatype output shape_out output shape","code":""},{"path":"/reference/nv_rng_state.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate random state — nv_rng_state","title":"Generate random state — nv_rng_state","text":"lightweight function generate initial state","code":""},{"path":"/reference/nv_rng_state.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate random state — nv_rng_state","text":"","code":"nv_rng_state(seed)"},{"path":"/reference/nv_rng_state.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate random state — nv_rng_state","text":"seed (integer(1)) Seed value","code":""},{"path":"/reference/nv_rng_state.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate random state — nv_rng_state","text":"nv_tensor dtype ui64 shape (2)","code":""},{"path":"/reference/nv_rnorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Random Normal Numbers — nv_rnorm","title":"Random Normal Numbers — nv_rnorm","text":"generate random normal numbers","code":""},{"path":"/reference/nv_rnorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random Normal Numbers — nv_rnorm","text":"","code":"nv_rnorm(initial_state, dtype, shape_out, mu = 0, sigma = 1)"},{"path":"/reference/nv_rnorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random Normal Numbers — nv_rnorm","text":"initial_state state seed dtype output dtype either \"f32\" \"f64\" shape_out output shape mu scalar: expected value sigma scalar: standard deviation #' @section Covariance: implement covariance structure use cholesky decomposition","code":""},{"path":"/reference/nv_runif.html","id":null,"dir":"Reference","previous_headings":"","what":"Random Uniform Numbers — nv_runif","title":"Random Uniform Numbers — nv_runif","text":"generate random uniform numbers ]lower, upper[ generate random uniform numbers [0, 1)","code":""},{"path":"/reference/nv_runif.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random Uniform Numbers — nv_runif","text":"","code":"nv_runif(initial_state, dtype = \"f64\", shape_out, lower = 0, upper = 1)  nv_unif_rand(initial_state, dtype = \"f64\", shape_out)"},{"path":"/reference/nv_runif.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random Uniform Numbers — nv_runif","text":"initial_state state seed dtype output dtype either \"f32\" \"f64\" shape_out output shape lower lower bound upper upper bound","code":""},{"path":"/reference/nv_select.html","id":null,"dir":"Reference","previous_headings":"","what":"Select — nv_select","title":"Select — nv_select","text":"return values true_value false_value conditioned pred","code":""},{"path":"/reference/nv_select.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select — nv_select","text":"","code":"nv_select(pred, true_value, false_value)"},{"path":"/reference/nv_select.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select — nv_select","text":"pred condition true_value true false_value false","code":""},{"path":"/reference/nv_select.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select — nv_select","text":"nv_tensor","code":""},{"path":"/reference/nv_slice.html","id":null,"dir":"Reference","previous_headings":"","what":"Slice — nv_slice","title":"Slice — nv_slice","text":"return slice operand.","code":""},{"path":"/reference/nv_slice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Slice — nv_slice","text":"","code":"nv_slice(operand, start_indices, limit_indices, strides)"},{"path":"/reference/nv_slice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Slice — nv_slice","text":"operand (nv_tensor) Operand. start_indices start slice limit_indices end slice strides stride size","code":""},{"path":"/reference/nv_slice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Slice — nv_slice","text":"nv_tensor","code":""},{"path":"/reference/nv_tensor.html","id":null,"dir":"Reference","previous_headings":"","what":"Tensor — AnvilTensor","title":"Tensor — AnvilTensor","text":"Create tensor.","code":""},{"path":"/reference/nv_tensor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tensor — AnvilTensor","text":"","code":"AnvilTensor  nv_tensor(data, dtype = NULL, device = NULL, shape = NULL)  nv_scalar(data, dtype = NULL, device = NULL)  nv_empty(dtype, shape, device = NULL)"},{"path":"/reference/nv_tensor.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Tensor — AnvilTensor","text":"object class S7_S3_class length 3.","code":""},{"path":"/reference/nv_tensor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tensor — AnvilTensor","text":"data () Object convertible PJRTBuffer. dtype (NULL | character(1) | stablehlo::TensorDataType) One pred, i8, i16, i32, i64, ui8, ui16, ui32, ui64, f32, f64 stablehlo::TensorDataType. default (NULL) uses f32 numeric data, i32 integer data, pred logical data. device (NULL | character(1) | PJRTDevice) platform name tensor (\"cpu\", \"cuda\", \"metal\"). Default use CPU, unless data already PJRTBuffer. can change default setting PJRT_PLATFORM environment variable. shape (NULL | integer()) Shape. default (NULL) infer data possible. Note nv_tensor interprets length 1 vectors shape (1). create \"scalar\" dimension (), use nv_scalar.","code":""},{"path":"/reference/nv_tensor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tensor — AnvilTensor","text":"(AnvilTensor)","code":""},{"path":"/reference/nv_tensor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Tensor — AnvilTensor","text":"Internally calls pjrt_buffer.","code":""},{"path":"/reference/nv_transpose.html","id":null,"dir":"Reference","previous_headings":"","what":"Transpose — nv_transpose","title":"Transpose — nv_transpose","text":"Transpose tensor.","code":""},{"path":"/reference/nv_transpose.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transpose — nv_transpose","text":"","code":"nv_transpose(x, permutation = NULL)  # S3 method for class '`anvil::GraphBox`' t(x)"},{"path":"/reference/nv_transpose.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transpose — nv_transpose","text":"x (nv_tensor) permutation (integer() | NULl) Permutation dimensions. NULL (default), reverses dimensions.","code":""},{"path":"/reference/nv_transpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transpose — nv_transpose","text":"nv_tensor","code":""},{"path":"/reference/nv_unary_ops.html","id":null,"dir":"Reference","previous_headings":"","what":"Unary Operations — nv_unary_ops","title":"Unary Operations — nv_unary_ops","text":"Unary operations tensors.","code":""},{"path":"/reference/nv_unary_ops.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unary Operations — nv_unary_ops","text":"","code":"nv_neg(operand)  nv_abs(operand)  nv_sqrt(operand)  nv_rsqrt(operand)  nv_log(operand)  nv_tanh(operand)  nv_tan(operand)  nv_sine(operand)  nv_cosine(operand)  nv_floor(operand)  nv_ceil(operand)  nv_sign(operand)  nv_exp(operand)  nv_round(operand, method = \"nearest_even\")"},{"path":"/reference/nv_unary_ops.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unary Operations — nv_unary_ops","text":"operand (nv_tensor) Operand. method (character(1)) Method use rounding. Either \"nearest_even\" (default) \"afz\" (away zero).","code":""},{"path":"/reference/nv_unary_ops.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unary Operations — nv_unary_ops","text":"nv_tensor","code":""},{"path":"/reference/nv_while.html","id":null,"dir":"Reference","previous_headings":"","what":"While — nv_while","title":"While — nv_while","text":"Functional loop.","code":""},{"path":"/reference/nv_while.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"While — nv_while","text":"","code":"nv_while(init, cond, body)"},{"path":"/reference/nv_while.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"While — nv_while","text":"init (list()) Initial state. cond (function) Condition function: f: state -> bool. body (function) Body function. f: state -> state.","code":""},{"path":"/reference/nv_while.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"While — nv_while","text":"nv_tensor","code":""},{"path":"/reference/platform.html","id":null,"dir":"Reference","previous_headings":"","what":"Platform — platform","title":"Platform — platform","text":"Get platform tensor-like object.","code":""},{"path":"/reference/platform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Platform — platform","text":"","code":"platform(x, ...)"},{"path":"/reference/platform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Platform — platform","text":"x () tensor. ... () Additional argument (unused).","code":""},{"path":"/reference/platform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Platform — platform","text":"(character(1))","code":""},{"path":"/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. stablehlo as_dtype, is_dtype, Shape tengen as_array, as_raw, device, dtype, ndims, shape","code":""},{"path":"/reference/stablehlo.html","id":null,"dir":"Reference","previous_headings":"","what":"Lower a function to StableHLO — stablehlo","title":"Lower a function to StableHLO — stablehlo","text":"Immediately lower flattened function StableHLO Func object.","code":""},{"path":"/reference/stablehlo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lower a function to StableHLO — stablehlo","text":"","code":"stablehlo(graph, constants_as_inputs = TRUE, env = NULL, donate = character())"},{"path":"/reference/stablehlo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lower a function to StableHLO — stablehlo","text":"graph (Graph) graph lower. constants_as_inputs (logical(1)) Whether add constants inputs. env (HloEnv | NULL) environment storing graph value func variable mappings. donate (character()) Names arguments whose buffers donated. Donated buffers can aliased outputs type.","code":""},{"path":"/reference/stablehlo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Lower a function to StableHLO — stablehlo","text":"(list) elements: func: StableHLO Func object constants: constants graph","code":""},{"path":"/reference/to_abstract.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to Abstract Tensor — to_abstract","title":"Convert to Abstract Tensor — to_abstract","text":"Convert object abstract tensor representation (AbstractTensor).","code":""},{"path":"/reference/to_abstract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to Abstract Tensor — to_abstract","text":"","code":"to_abstract(x)"},{"path":"/reference/to_abstract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to Abstract Tensor — to_abstract","text":"x () Object convert.","code":""},{"path":"/reference/to_abstract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert to Abstract Tensor — to_abstract","text":"AbstractTensor","code":""},{"path":"/reference/trace_fn.html","id":null,"dir":"Reference","previous_headings":"","what":"Trace an R function into a Graph — trace_fn","title":"Trace an R function into a Graph — trace_fn","text":"Create graph representation R function tracing.","code":""},{"path":"/reference/trace_fn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trace an R function into a Graph — trace_fn","text":"","code":"trace_fn(f, args, desc = NULL)"},{"path":"/reference/trace_fn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trace an R function into a Graph — trace_fn","text":"f (function) function trace_fn. args (list) arguments function. desc (NULL | GraphDescriptor) descriptor use graph.","code":""},{"path":"/reference/trace_fn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trace an R function into a Graph — trace_fn","text":"(Graph)","code":""}]
